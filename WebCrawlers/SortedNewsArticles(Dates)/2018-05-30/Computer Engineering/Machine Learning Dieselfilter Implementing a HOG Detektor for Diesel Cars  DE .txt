Wir zeigen euch in dieser Artikelreihe anhand des Beispiels Erkennung von Umweltplaketten an Fahrzeugen unterschiedliche Machine- und Deep-Learning-Anstze. In diesem Artikel implementieren wir einen Detektor auf Basis des HOG-Algorithmus (Histogram of Oriented Gradients) aus der dlib C++ Library.

Die Links zu allen Teilen und unserem youtube channel finden sich hier:

 >Deep Diesel Teil 1: Machine und Deep Learning zur Erkennung von Umweltplaketten

 >Deep Diesel  Teil 2: Machine-Learning-Dieselfilter HOG Detektor 

 >codecentric.ai youtube channel fr regelmige Machine- und Deeplearning Updates

Ein schneller Einstieg in die Arbeit mit Machine Learning im Bereich Computer Vision gelingt mit der dlib Library, die eine Vielzahl von Algorithmen zusammenfasst. Die Library ist eine C++-basierte Bibliothek, bietet aber fr ein Subset der Funktionen auch ein Python Interface an. http://dlib.net/. Ein Blick in das sehr breite Spektrum an Beispielen und Kommentaren, die mit der Library ausgeliefert werden, lohnt sich auf jeden Fall: Von Gesichtserkennung ber Fahrzeug-Detektion bis hin zum Deep-Learning-basierten Dog-Hipsterizer finden sich unterschiedliche Szenarien in den Beispielen.

Eine gute Hilfe zur Installation findet sich hier.Ihr solltet idealerweise die dlib mit GPU-Untersttzung nutzen (CUDA/cuDNN).

Wir haben uns fr den Start fr einen einen HOG-Detektor (Histogram of Oriented Gradients) entschieden, der sich primr auf die geometrischen Eigenschaften des Objekts bezieht. Der groe Vorteil an diesem Detektor ist, dass fr erste Ergebnisse nur sehr wenige Trainingsdaten bentigt werden. Die grundlegende Idee ist eine lokalisierte Auswertung (Zhlung) des Auftreten von Gradienten (Kanten) und die Zusammenfassung in ein charakteristisches Histogramm. Dieses Histogramm kann dann als Template genutzt werden, um hnliche Strukturen in anderen Bildern zu identifizieren. Lokalisiert bedeutet in diesem Kontext, dass viele Histogramme entlang eines engen Gitters fr das Bild erstellt und ausgewertet werden.

Dieses Vorgehen ermglicht es, geometrische Objekte auch nach Verschiebung, in Grenzen auch bei Skalierung, zu erkennen die Erkennung ist aber leider nicht Rotations- oder Perspektiv-invariant, da sich die Orientierung der Gradienten im auszuwertenden Bild verndert.

Der HOG-Detektor macht in unserem Use Case keinen Gebrauch von einem naheliegenden Charakteristikum  der Farbe der Umweltplakette  sondern orientiert sich ausschlielich an der runden Form und der kontraststarken 4. Um einen Eindruck zu bekommen, woran sich der Algorithmus orientiert, kann man die Feature-Gradienten visualisieren.

Mit der folgenden Methode extrahieren wir die Features unserem Bild:

Der entsprechende Call der Methode, um 32 Pixel Blcke zu visualisieren, ist:

Ein guter Ausgangspunkt, um dies fr eigene Bilder und Objekte nachzuvollziehen, ist das mit der dlib ausgelieferte Beispiel: fhog_ex.cpp http://dlib.net/fhog_ex.cpp.html, die leicht gepatched werden kann. Hier einige Beispiele fr die erkannten Features der Umweltplakette.

Fr die Umsetzung des Detektors verwenden wir das Python Interface und orientieren uns an typischen Schritten zur Erstellung eines Detektors. Den Weg zu einem funktionierenden Detektor unterteilen wir in fnf Schritte.

Bei der Arbeit am Detektor haben wir mehrere Methoden zum Sammeln und den Einfluss auf die Leistung des Detektors ausprobiert.

Initial haben wir ohne die Bercksichtigung der Eigenschaften des Algorithmus Frontalfotos von parkenden Fahrzeugen gemacht, spter haben wir die Perspektive angepasst und zuletzt sind wir auf die Nutzung von Videos von Fahrzeugen in Bewegung umgestiegen. Die Performance der Ergebnisse ist von Schritt zu Schritt besser geworden, wobei der grte Unterschied beim Umstieg auf die Nutzung der Videos zu erkennen war. Insbesondere die korrekte Perspektive und das Nutzen des gleichen Devices fr die Aufnahme von Trainingsdaten, Testdaten und spterer Detektion haben sich als die Faktoren mit dem grten Einfluss herausgestellt. Das einfachste Vorgehen fr die Erhebung von Testdaten ist es, ein Video von vorbeifahrenden Fahrzeugen aufzunehmen und dann ausgewhlte Frames, auf denen die Umweltplakette gut erkennbar ist, zu speichern.

Des Weiteren ist zu bemerken, dass es beim HOG-Detektor eher darauf ankommt, wenige reprsentative Bilder zu finden als viele unterschiedliche Trainingsdaten. Viele Bilder verwssern in diesem Fall das Histogramm, nach dem wir in den Testdaten suchen, und machen das Ergebnis eher schlechter als besser.

Zum Labeling der Daten verwenden wir das Tool imglab, welches nach dem Kompilieren der dlib C++ Library im dlib/tools/ Ordner zu finden ist. Mit dem Befehl

wird initial ein XML File mit der Liste der Trainingsdaten generiert, welches danach mit

geladen wird, um die Label anzubringen.

Nach der Eingabe des Label-Namens werden die Regions of Interest (RoIs) umrandet. Es hat sich gezeigt, dass Objekte nicht pixelgenau eingekreist werden sollten, da die Gradienten innerhalb der Markierung als charakteristische Merkmale interpretiert werden. Ergebnis ist ein XML File mit den Pfaden zu den Daten und den Koordinaten zu den RoIs.

Beispiel fr ein XML File mit den Metainformationen zu den Trainingsdaten

Zum Training des Classifiers erstellen wir ein minimales Python-Script, in welchem wir das XML File mit den Labels einlesen und die notwendige Parametrisierung vornehmen (training_options:http://dlib.net/python/index.html#dlib.train_simple_object_detector).

Dabei knnen wir optional die Trainingsdaten verdoppeln (Data Augmentation), indem wir Bilder fr spiegelsymmetrische Objekte vertikal spiegeln (auf Schriftzeichen, wie in unserem Fall die 4, nicht anwendbar). Entsprechend der Maschine, auf der wir trainieren, knnen wir die Anzahl der Threads anpassen, die fr die Parallelisierung des Trainings gespawnt werden. Der Parameter C entspricht dem SVM C regularization parameter, der ein Ma dafr ist, wie stark der Trainings-Algorithmus Fehlklassifikationen bestrafen soll. Je hher C, desto enger werden die Klassifikationsgrenzen gezogen, was am Ende zu einem Overfitting in Bezug auf die Trainingsdaten fhren kann. Ein passendes C ist durch geschicktes Ausprobieren zu finden. Eine gute Darstellung des Effektes findet sich hier.

Ein grundlegende Methodik im Machine Learning ist das Aufteilen der vorliegenden gelabelten Bilder in Trainings- und Testsets. Dabei wird mit den Bildern des Trainingssets der Detektor trainiert und die Performance ber das Klassifizieren der Bilder aus dem Testset gemessen.

Precision ist eine Metrik zur Messung der Genauigkeit der Detektionen  eine Precision von 1 heit, dass es keine falsch positiven (falscher Alarm) Detektionen gab. Eine Precision von 0 heit, es wurden in jedem Bild falsche Detektionen vorgenommen.

Recall benennt die Wahrscheinlichkeit der korrekt detektierten Objekte. Ein Recall von 1 entspricht einem Detektor, der alle Objekte gefunden hat. Ein Recall von 0 heit, es wurde kein Objekt gefunden.

Average Precision ist ein Ma fr die Gesamt-Gte des Detektors, welches ber alle Detektionen gerechnet wird. Je nher an 1, desto besser ist der Detektor auf dem Test-Set. Eine detaillierte Beschreibung findet sich hier.

Ziel des Szenarios war es, eine Real-Time-Analyse des Verkehrsflusses vorzunehmen, also auf einem Video-Live-Stream die Detektion durchzufhren. Dafr haben wir ein einfaches Setup mit gewhlt, was einfach reproduzierbar ist.

Um eine Auswertung der Live-Daten des Verkehrs zu bekommen, bauen wir eine kleine Applikation, die einen Videostream nimmt und die Einzelbilder durch unseren Detektor laufen lsst.

Da die Frames live ausgewertet werden sollen und aktuelle Bilder mit cam.read() immer nur zwischen den Auswertungen mit dem Detektor aktualisiert werden, ist die Frame-Rate eher gering. Fr eine flssige Auswertung kann ein pre-recorded Video Frame fr Frame durch den Detektor ausgewertet werden.

Der von uns vorgestellte einfache Detektor ist leicht zu implementieren, hat aber im vorliegenden Setup in Bezug auf das Anwendungsszenario nur eine ausreichende Performance. Um diese zu steigern, wrde noch einiges an Aufwand in die Parametrisierung des Algorithmus flieen mssen. Insbesondere Aspekte wie die starke Abhngigkeit von der Perspektive sind eine Herausforderung. Im Versuchsaufbau sind mit steigender Geschwindigkeit auch Aspekte wie Bewegungsunschrfe und niedrige Frame-Rate ein Problem, denn der Detektor ist allein auf klare Umrisse angewiesen und darauf, dass die Umweltplakette in ausreichender Gre erkennbar ist. Diese Aspekte lassen sich sicherlich mit verbesserter Hardware in Hinblick auf optische Qualitt und Rechenleistung adressieren. Da der Detektor keine Informationen aus den Farbkanlen zieht, wre es auch mglich, in den Infrarotbereich auszuweichen und Einzelaufnahmen mit Blitz zu machen, wie es bei den aktuell in der Verkehrsberwachung eingesetzten Gerten schon der Fall ist.

Fr einfache Object-Detection-Szenarien in kontrollierten Umgebungen ist der Algorithmus sehr gut geeignet, um schnell zuverlssige Ergebnisse zu liefern. Insbesondere positiv hervorzuheben ist die Eigenschaft, dass nur sehr wenige Trainingsdaten bentigt werden.

Im nchsten Setup wollen wir uns die Eigenschaften und Performance von Detektoren anschauen, die auf Deep-Learning-Anstzen basieren.

Die Links zu allen Teilen finden sich hier:

 >Deep Diesel Teil 1: Machine und Deep Learning zur Erkennung von Umweltplaketten

 >Deep Diesel  Teil 2: Machine-Learning-Dieselfilter HOG Detektor

 >codecentric.ai youtube channel fr regelmige Machine- und Deeplearning Updates